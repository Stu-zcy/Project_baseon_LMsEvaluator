LM_config:
  local_model: true
  model: bert_base_uncased
attack_list:
- attack_args:
    attack: false
    attack_nums: 20
    attack_recipe: TextFoolerJin2019
    attack_type: AdvAttack
    dataset_name_or_path: datasets/imdb/train.txt
    defender: false
    display_full_info: true
    model_name_or_path: LMs/bert_base_uncased
    tokenizer_name_or_path: LMs/bert_base_uncased
    use_local_dataset: true
    use_local_model: true
    use_local_tokenizer: true
- attack_args:
    attack: false
    attack_nums: 3
    attack_recipe: TextFoolerJin2019
    attack_type: AdvAttack
    dataset_name_or_path: datasets/imdb/train.txt
    defender:
      gradient_accumulation_steps: 4
      learning_rate: 5.0e-05
      log_to_tb: false
      num_clean_epochs: 0
      num_epochs: 1
      num_train_adv_examples: 10
      per_device_train_batch_size: 8
    display_full_info: true
    model_name_or_path: LMs/bert_base_uncased
    tokenizer_name_or_path: LMs/bert_base_uncased
    use_local_dataset: true
    use_local_model: true
    use_local_tokenizer: true
- attack_args:
    attack: false
    attack_type: BackdoorAttack
    defender: None
    display_full_info: true
    model: bert
    model_name_or_path: LMs/bert_base_uncased
    poison_dataset: sst-2
    poisoner:
      name: badnets
    sample_metrics:
    - ppl
    - use
    target_dataset: sst-2
    train:
      batch_size: 32
      epochs: 1
      name: base
    use_local_model: true
- attack_args:
    attack: true
    attack_type: PoisoningAttack
    poisoning_rate: 10
    save_path: ./attack/PoisoningAttack/model_output
    train_config:
      logging_dir: ./logs
      logging_steps: 1000
      num_train_epochs: 10
      output_dir: ./attack/PoisoningAttack/cache
      per_device_eval_batch_size: 64
      per_device_train_batch_size: 16
      report_to: none
      run_name: my_experiment
      warmup_steps: 1000
      weight_decay: 0.01
- attack_args:
    attack: false
    attack_type: PoisoningAttack
    poisoning_rate: 0.7
    save_path: ./attack/PoisoningAttack/model_output
    train_config:
      logging_dir: ./logs
      logging_steps: 1000
      num_train_epochs: 10
      output_dir: ./attack/PoisoningAttack/cache
      per_device_eval_batch_size: 64
      per_device_train_batch_size: 16
      report_to: none
      run_name: my_experiment
      warmup_steps: 1000
      weight_decay: 0.01
- attack_args:
    attack: false
    attack_batch: 2
    attack_nums: 1
    attack_type: FET
    crossover_rate: 0.9
    dataset_name_or_path: cola
    display_full_info: true
    distance_func: l2
    halloffame_size: 30
    max_generations: 2
    model_name_or_path: LMs/bert_base_uncased
    mutation_rate: 0.1
    population_size: 300
    seed: 42
    tokenizer_name_or_path: LMs/bert_base_uncased
    tournsize: 5
    use_local_model: true
    use_local_tokenizer: true
- attack_args:
    al_sample_batch_num: -1
    al_sample_method: random
    attack: false
    attack_type: ModelStealingAttack
    epsilon: -1
    initial_drk_model: None
    initial_sample_method: random_sentence
    method: MeaeQ
    pool_data_source: wiki
    pool_data_type: whole
    pool_subsize: -1
    prompt: None
    query_num: 500
    run_seed_arr:
    - 56
- attack_args:
    attack: false
    attack_type: RLMI
    dataset_name: emotion
    max_input_length: 5
    max_iterations: 2500
    min_input_length: 2
    model_name: tinybert4
    num_generation: 1000
    ppo_config:
      batch_size: 16
      learning_rate: 1.0e-05
      log_with: None
      mini_batch_size: 16
    seed: 123
    seq_length: 20
    target_label: 0
general:
  log_file_name: ChenyangZhao_single_1753502792
  logs_save_dir: ./logs
  random_seed: 42
  use_gpu: false
output:
  base_path: output
  evaluation_result: evaluationResult
  model_output: modelOutput
task_config:
  dataset: imdb
  local_dataset: true
  normal_training: true
  save_path: ./cache/model_output
  task: TaskForSingleSentenceClassification
  train_config:
    logging_dir: ./logs
    logging_steps: 1000
    num_train_epochs: 30
    output_dir: ./cache
    per_device_eval_batch_size: 64
    per_device_train_batch_size: 16
    report_to: none
    run_name: my_experiment
    warmup_steps: 1000
    weight_decay: 0.01
