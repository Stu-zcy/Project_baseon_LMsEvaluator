{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "177fa5c7-b856-4a39-85df-1a3dda4f0daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-12 13:26:27] - INFO: 成功导入BERT配置文件 /Users/zkzhu/Project/PycharmProject/LMsEvaluation/LMs/bert_base_chinese/config.json\n",
      "[2024-04-12 13:26:27] - INFO:  ### 将当前配置打印到日志文件中 \n",
      "[2024-04-12 13:26:27] - INFO: ###  project_dir = /Users/zkzhu/Project/PycharmProject/LMsEvaluation\n",
      "[2024-04-12 13:26:27] - INFO: ###  dataset_dir = /Users/zkzhu/Project/PycharmProject/LMsEvaluation/datasets/imdb\n",
      "[2024-04-12 13:26:27] - INFO: ###  pretrained_model_dir = /Users/zkzhu/Project/PycharmProject/LMsEvaluation/LMs/bert_base_chinese\n",
      "[2024-04-12 13:26:27] - INFO: ###  vocab_path = /Users/zkzhu/Project/PycharmProject/LMsEvaluation/LMs/bert_base_chinese/vocab.txt\n",
      "[2024-04-12 13:26:27] - INFO: ###  device = mps\n",
      "[2024-04-12 13:26:27] - INFO: ###  train_file_path = /Users/zkzhu/Project/PycharmProject/LMsEvaluation/datasets/imdb/train.txt\n",
      "[2024-04-12 13:26:27] - INFO: ###  val_file_path = /Users/zkzhu/Project/PycharmProject/LMsEvaluation/datasets/imdb/val.txt\n",
      "[2024-04-12 13:26:27] - INFO: ###  test_file_path = /Users/zkzhu/Project/PycharmProject/LMsEvaluation/datasets/imdb/test.txt\n",
      "[2024-04-12 13:26:27] - INFO: ###  model_save_dir = /Users/zkzhu/Project/PycharmProject/LMsEvaluation/cache\n",
      "[2024-04-12 13:26:27] - INFO: ###  logs_save_dir = /Users/zkzhu/Project/PycharmProject/LMsEvaluation/logs\n",
      "[2024-04-12 13:26:27] - INFO: ###  model_save_path = /Users/zkzhu/Project/PycharmProject/LMsEvaluation/cache/model.pt\n",
      "[2024-04-12 13:26:27] - INFO: ###  config_parser = {'general': {'random_seed': 0, 'use_gpu': True}, 'LM_config': {'model': 'bert_base_chinese'}, 'task_config': {'task': 'TaskForSingleSentenceClassification', 'dataset': 'imdb', 'dataset_type': '.txt', 'split_sep': '_!_', 'epochs': 10}, 'attack_list': [{'attack_args': {'attack': True, 'attack_type': 'AdvAttack', 'attack_recipe': 'TextFoolerJin2019', 'use_local_model': True, 'use_local_tokenizer': True, 'use_local_dataset': True, 'model_name_or_path': 'LMs/bert_base_uncased', 'tokenizer_name_or_path': 'LMs/bert_base_uncased', 'dataset_name_or_path': 'datasets/imdb/train.txt', 'attack_nums': 2}}, {'attack_args': {'attack': True, 'attack_type': 'SWAT', 'seed': 42, 'attack_batch': 2, 'attack_nums': 1, 'distance_func': 'l2', 'population_size': 300, 'tournsize': 5, 'crossover_rate': 0.9, 'mutation_rate': 0.1, 'max_generations': 2, 'halloffame_size': 30, 'use_local_model': True, 'use_local_tokenizer': True, 'model_name_or_path': 'LMs/bert_base_uncased', 'tokenizer_name_or_path': 'LMs/bert_base_uncased', 'dataset_name_or_path': 'cola'}}, {'attack_args': {'attack': True, 'attack_type': 'BackDoorAttack', 'use_local_model': True, 'model': 'bert', 'model_name_or_path': 'LMs/bert_base_uncased', 'poison_dataset': 'sst-2', 'target_dataset': 'sst-2', 'poisoner': {'name': 'badnets'}, 'train': {'name': 'base', 'batch_size': 32, 'epochs': 1}, 'defender': 'None'}}, {'attack_args': {'attack': True, 'attack_type': 'PoisoningAttack', 'poisoning_rate': 0.1}}], 'output': {'base_path': 'output', 'model_output': 'modelOutput', 'evaluation_result': 'evaluationResult'}}\n",
      "[2024-04-12 13:26:27] - INFO: ###  log_level = 20\n",
      "[2024-04-12 13:26:27] - INFO: ###  vocab_size = 21128\n",
      "[2024-04-12 13:26:27] - INFO: ###  hidden_size = 768\n",
      "[2024-04-12 13:26:27] - INFO: ###  num_hidden_layers = 12\n",
      "[2024-04-12 13:26:27] - INFO: ###  num_attention_heads = 12\n",
      "[2024-04-12 13:26:27] - INFO: ###  intermediate_size = 3072\n",
      "[2024-04-12 13:26:27] - INFO: ###  pad_token_id = 0\n",
      "[2024-04-12 13:26:27] - INFO: ###  hidden_act = gelu\n",
      "[2024-04-12 13:26:27] - INFO: ###  hidden_dropout_prob = 0.1\n",
      "[2024-04-12 13:26:27] - INFO: ###  attention_probs_dropout_prob = 0.1\n",
      "[2024-04-12 13:26:27] - INFO: ###  max_position_embeddings = 512\n",
      "[2024-04-12 13:26:27] - INFO: ###  type_vocab_size = 2\n",
      "[2024-04-12 13:26:27] - INFO: ###  initializer_range = 0.02\n",
      "[2024-04-12 13:26:27] - INFO: ###  split_sep = _!_\n",
      "[2024-04-12 13:26:27] - INFO: ###  is_sample_shuffle = True\n",
      "[2024-04-12 13:26:27] - INFO: ###  batch_size = 1\n",
      "[2024-04-12 13:26:27] - INFO: ###  max_sen_len = None\n",
      "[2024-04-12 13:26:27] - INFO: ###  num_labels = 15\n",
      "[2024-04-12 13:26:27] - INFO: ###  epochs = 10\n",
      "[2024-04-12 13:26:27] - INFO: ###  model_val_per_epoch = 2\n",
      "[2024-04-12 13:26:27] - INFO: ###  directionality = bidi\n",
      "[2024-04-12 13:26:27] - INFO: ###  pooler_fc_size = 768\n",
      "[2024-04-12 13:26:27] - INFO: ###  pooler_num_attention_heads = 12\n",
      "[2024-04-12 13:26:27] - INFO: ###  pooler_num_fc_layers = 3\n",
      "[2024-04-12 13:26:27] - INFO: ###  pooler_size_per_head = 128\n",
      "[2024-04-12 13:26:27] - INFO: ###  pooler_type = first_token_transform\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zkzhu/anaconda3/envs/LMs3.9env/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-12 13:26:29] - INFO: ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现，如需使用torch框架中的MultiHeadAttention模块可通过config.__dict__['use_torch_multi_head'] = True实现\n",
      "[2024-04-12 13:26:29] - INFO:  ## 索引预处理缓存文件的参数为：['max_sen_len']\n",
      "[2024-04-12 13:26:29] - INFO: 缓存文件 /Users/zkzhu/Project/PycharmProject/LMsEvaluation/datasets/imdb/cache_test_max_sen_lenNone.pt 存在，直接载入缓存文件！\n",
      "[2024-04-12 13:26:29] - INFO: 数据预处理一共耗时0.004s\n",
      "[2024-04-12 13:26:29] - INFO:  ## 索引预处理缓存文件的参数为：['max_sen_len']\n",
      "[2024-04-12 13:26:29] - INFO: 缓存文件 /Users/zkzhu/Project/PycharmProject/LMsEvaluation/datasets/imdb/cache_train_max_sen_lenNone.pt 存在，直接载入缓存文件！\n",
      "[2024-04-12 13:26:30] - INFO: 数据预处理一共耗时0.769s\n",
      "[2024-04-12 13:26:30] - INFO:  ## 索引预处理缓存文件的参数为：['max_sen_len']\n",
      "[2024-04-12 13:26:30] - INFO: 缓存文件 /Users/zkzhu/Project/PycharmProject/LMsEvaluation/datasets/imdb/cache_val_max_sen_lenNone.pt 存在，直接载入缓存文件！\n",
      "[2024-04-12 13:26:31] - INFO: 数据预处理一共耗时0.822s\n",
      "[2024-04-12 13:26:31] - INFO: Epoch: 0, Batch[0/25000], Train loss :2.704, Train acc: 0.000\n",
      "[2024-04-12 13:26:31] - INFO: Epoch: 0, Train loss: 0.000, Epoch time = 0.571s\n",
      "[2024-04-12 13:26:32] - INFO: Checking the config of AdvAttack.\n",
      "[2024-04-12 13:26:32] - INFO: ==========================================================\n",
      "[2024-04-12 13:26:32] - INFO: 攻击开始\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zkzhu/anaconda3/envs/LMs3.9env/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /Users/zkzhu/Project/PycharmProject/LMsEvaluation/LMs/bert_base_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "textattack: Unknown if model of class <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.\n",
      "textattack: Logging to CSV at path /Users/zkzhu/Project/PycharmProject/LMsEvaluation/attack/AdvAttack/log.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack(\n",
      "  (search_method): GreedyWordSwapWIR(\n",
      "    (wir_method):  delete\n",
      "  )\n",
      "  (goal_function):  UntargetedClassification\n",
      "  (transformation):  WordSwapEmbedding(\n",
      "    (max_candidates):  50\n",
      "    (embedding):  WordEmbedding\n",
      "  )\n",
      "  (constraints): \n",
      "    (0): WordEmbeddingDistance(\n",
      "        (embedding):  WordEmbedding\n",
      "        (min_cos_sim):  0.5\n",
      "        (cased):  False\n",
      "        (include_unknown_words):  True\n",
      "        (compare_against_original):  True\n",
      "      )\n",
      "    (1): PartOfSpeech(\n",
      "        (tagger_type):  nltk\n",
      "        (tagset):  universal\n",
      "        (allow_verb_noun_swap):  True\n",
      "        (compare_against_original):  True\n",
      "      )\n",
      "    (2): UniversalSentenceEncoder(\n",
      "        (metric):  angular\n",
      "        (threshold):  0.840845057\n",
      "        (window_size):  15\n",
      "        (skip_text_shorter_than_window):  True\n",
      "        (compare_against_original):  False\n",
      "      )\n",
      "    (3): RepeatModification\n",
      "    (4): StopwordModification\n",
      "    (5): InputColumnModification(\n",
      "        (matching_column_labels):  ['premise', 'hypothesis']\n",
      "        (columns_to_ignore):  {'premise'}\n",
      "      )\n",
      "  (is_black_box):  True\n",
      ") \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                               | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-12 13:27:04] - INFO: Using /var/folders/dr/nw8_6x6j4fv59rr15xft1m940000gn/T/tfhub_modules to cache modules.\n",
      "[2024-04-12 13:27:06] - INFO: Fingerprint not found. Saved model loading will continue.\n",
      "[2024-04-12 13:27:06] - INFO: path_and_singleprint metric could not be logged. Saved model loading will continue.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 2 / 0 / 0 / 2: 100%|█████████████████████████████████████████████████████████████████████████████████| 2/2 [02:57<00:00, 88.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "+-------------------------------+--------+\n",
      "| Attack Results                |        |\n",
      "+-------------------------------+--------+\n",
      "| Number of successful attacks: | 2      |\n",
      "| Number of failed attacks:     | 0      |\n",
      "| Number of skipped attacks:    | 0      |\n",
      "| Original accuracy:            | 100.0% |\n",
      "| Accuracy under attack:        | 0.0%   |\n",
      "| Attack success rate:          | 100.0% |\n",
      "| Average perturbed word %:     | 7.16%  |\n",
      "| Average num. words per input: | 253.0  |\n",
      "| Avg num queries:              | 533.0  |\n",
      "+-------------------------------+--------+\n",
      "[2024-04-12 13:29:34] - INFO: 攻击结束\n",
      "[2024-04-12 13:29:34] - INFO: ==========================================================\n",
      "[2024-04-12 13:29:34] - INFO: Checking the config of SWAT.\n",
      "[2024-04-12 13:29:34] - INFO: ==========================================================\n",
      "[2024-04-12 13:29:34] - INFO: 攻击开始\n",
      "[2024-04-12 13:29:34] - INFO: Namespace(seed=42, dataset='cola', model='tinybert', batch_size=2, n_attacks=1, distance_function='l2', population_size=300, tournsize=5, crossover_rate=0.9, mutation_rate=0.1, max_generations=2, halloffame_size=30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /Users/zkzhu/Project/PycharmProject/LMsEvaluation/LMs/bert_base_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-12 13:29:36] - INFO: ----------------------------------------------------------------------\n",
      "[2024-04-12 13:29:36] - INFO: Attack No.1:\n",
      "[2024-04-12 13:29:36] - INFO: Reference = ['[CLS] the party lasted till midnight. [SEP] [PAD] [PAD] [PAD]', '[CLS] the shooting of the hunters was very loud. [SEP]']\n",
      "gen\tnevals\tmin    \tavg    \n",
      "0  \t300   \t28.4278\t1813.75\n",
      "1  \t242   \t22.5984\t1741.92\n",
      "2  \t250   \t20.9153\t1519.87\n",
      "3  \t234   \t20.9153\t929.097\n",
      "[2024-04-12 13:30:54] - INFO: Prediction = ['[CLS] the till very was was lasted midnight. [PAD] [SEP]', '[CLS] loud till shooting lasted party of midnight hunters lasted [SEP]']\n",
      "[2024-04-12 13:30:54] - INFO: Generations = 3\n",
      "[2024-04-12 13:30:58] - INFO: Using default tokenizer.\n",
      "[2024-04-12 13:30:58] - INFO: Rouge scores: {'rouge1': 0.6357142857142857, 'rouge2': 0.05555555555555555, 'rougeL': 0.4880952380952381, 'rougeLsum': 0.4880952380952381}\n",
      "[2024-04-12 13:30:58] - INFO: Word recovery rate: 1.0\n",
      "[2024-04-12 13:30:58] - INFO: Edit distance: 71\n",
      "[2024-04-12 13:30:58] - INFO: If full recovery: False\n",
      "[2024-04-12 13:30:58] - INFO: Using default tokenizer.\n",
      "[2024-04-12 13:30:58] - INFO: Aggregate rouge scores: {'rouge1': 0.6357142857142857, 'rouge2': 0.05555555555555555, 'rougeL': 0.4880952380952381, 'rougeLsum': 0.4880952380952381}\n",
      "[2024-04-12 13:30:58] - INFO: Full recovery rate: 0.0\n",
      "[2024-04-12 13:30:58] - INFO: Average word recovery rate: 1.0\n",
      "[2024-04-12 13:30:58] - INFO: Average edit distance: 71.0\n",
      "[2024-04-12 13:30:58] - INFO: 攻击结束\n",
      "[2024-04-12 13:30:58] - INFO: ==========================================================\n",
      "[2024-04-12 13:30:58] - INFO: Checking the config of BackDoorAttack.\n",
      "[2024-04-12 13:30:58] - INFO: ==========================================================\n",
      "[2024-04-12 13:30:58] - INFO: 攻击开始\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zkzhu/anaconda3/envs/LMs3.9env/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /Users/zkzhu/Project/PycharmProject/LMsEvaluation/LMs/bert_base_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[\u001b[032m2024-04-12 13:31:02,294\u001b[0m INFO] badnets_poisoner Initializing BadNet poisoner, triggers are cf mn bb tq\n",
      "[\u001b[032m2024-04-12 13:31:02,317\u001b[0m INFO] __init__ sst-2 dataset loaded, train: 6920, dev: 872, test: 1821\n",
      "/Users/zkzhu/anaconda3/envs/LMs3.9env/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[\u001b[032m2024-04-12 13:31:02,384\u001b[0m INFO] trainer ***** Training *****\n",
      "[\u001b[032m2024-04-12 13:31:02,385\u001b[0m INFO] trainer   Num Epochs = 1\n",
      "[\u001b[032m2024-04-12 13:31:02,385\u001b[0m INFO] trainer   Instantaneous batch size per GPU = 32\n",
      "[\u001b[032m2024-04-12 13:31:02,385\u001b[0m INFO] trainer   Gradient Accumulation steps = 1\n",
      "[\u001b[032m2024-04-12 13:31:02,385\u001b[0m INFO] trainer   Total optimization steps = 217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 217/217 [04:26<00:00,  1.23s/it]\n",
      "[\u001b[032m2024-04-12 13:35:29,326\u001b[0m INFO] trainer Epoch: 1, avg loss: 0.6178113529759068\n",
      "[\u001b[032m2024-04-12 13:35:29,326\u001b[0m INFO] eval ***** Running evaluation on dev-clean *****\n",
      "Evaluating: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28/28 [00:08<00:00,  3.46it/s]\n",
      "[\u001b[032m2024-04-12 13:35:37,422\u001b[0m INFO] eval   Num examples = 872\n",
      "[\u001b[032m2024-04-12 13:35:37,427\u001b[0m INFO] eval   accuracy on dev-clean: 0.8669724770642202\n",
      "[\u001b[032m2024-04-12 13:35:37,427\u001b[0m INFO] eval ***** Running evaluation on dev-poison *****\n",
      "Evaluating: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 14/14 [00:04<00:00,  3.38it/s]\n",
      "[\u001b[032m2024-04-12 13:35:41,574\u001b[0m INFO] eval   Num examples = 444\n",
      "[\u001b[032m2024-04-12 13:35:41,575\u001b[0m INFO] eval   accuracy on dev-poison: 0.15990990990990991\n",
      "[\u001b[032m2024-04-12 13:35:42,234\u001b[0m INFO] trainer Training finished.\n",
      "[\u001b[032m2024-04-12 13:35:42,376\u001b[0m INFO] __init__ sst-2 dataset loaded, train: 6920, dev: 872, test: 1821\n",
      "[\u001b[032m2024-04-12 13:35:42,378\u001b[0m INFO] eval ***** Running evaluation on test-clean *****\n",
      "Evaluating: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 57/57 [00:17<00:00,  3.25it/s]\n",
      "[\u001b[032m2024-04-12 13:35:59,922\u001b[0m INFO] eval   Num examples = 1821\n",
      "[\u001b[032m2024-04-12 13:35:59,923\u001b[0m INFO] eval   accuracy on test-clean: 0.8813838550247117\n",
      "[\u001b[032m2024-04-12 13:35:59,924\u001b[0m INFO] eval ***** Running evaluation on test-poison *****\n",
      "Evaluating: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:08<00:00,  3.27it/s]\n",
      "[\u001b[032m2024-04-12 13:36:08,788\u001b[0m INFO] eval   Num examples = 909\n",
      "[\u001b[032m2024-04-12 13:36:08,789\u001b[0m INFO] eval   accuracy on test-poison: 0.16501650165016502\n",
      "[\u001b[032m2024-04-12 13:36:08,844\u001b[0m INFO] TaskForSingleSentenceClassification 攻击结束\n",
      "[\u001b[032m2024-04-12 13:36:08,844\u001b[0m INFO] TaskForSingleSentenceClassification ==========================================================\n",
      "[\u001b[032m2024-04-12 13:36:08,845\u001b[0m INFO] attack_helper Checking the config of PoisoningAttack\n",
      "[\u001b[032m2024-04-12 13:36:08,845\u001b[0m INFO] main 生在生成投毒数据\n",
      "[\u001b[032m2024-04-12 13:36:08,845\u001b[0m INFO] main 原始数据集路径: /Users/zkzhu/Project/PycharmProject/LMsEvaluation/datasets/imdb/train.txt\n",
      "[\u001b[032m2024-04-12 13:36:08,846\u001b[0m INFO] main 投毒数据集路径: /Users/zkzhu/Project/PycharmProject/LMsEvaluation/datasets/imdb/poisoning_train.txt\n",
      "[\u001b[032m2024-04-12 13:36:09,913\u001b[0m INFO] main 投毒数据生成完毕\n",
      "[\u001b[032m2024-04-12 13:36:09,921\u001b[0m INFO] TaskForSingleSentenceClassification ==========================================================\n",
      "[\u001b[032m2024-04-12 13:36:09,922\u001b[0m INFO] TaskForSingleSentenceClassification 攻击开始\n",
      "[\u001b[032m2024-04-12 13:36:09,926\u001b[0m INFO] data_helpers  ## 索引预处理缓存文件的参数为：['max_sen_len']\n",
      "[\u001b[032m2024-04-12 13:36:09,927\u001b[0m INFO] data_helpers 缓存文件 /Users/zkzhu/Project/PycharmProject/LMsEvaluation/datasets/imdb/cache_test_max_sen_lenNone.pt 存在，直接载入缓存文件！\n",
      "[\u001b[032m2024-04-12 13:36:09,931\u001b[0m INFO] data_helpers 数据预处理一共耗时0.004s\n",
      "[\u001b[032m2024-04-12 13:36:09,931\u001b[0m INFO] data_helpers  ## 索引预处理缓存文件的参数为：['max_sen_len']\n",
      "[\u001b[032m2024-04-12 13:36:09,931\u001b[0m INFO] data_helpers 缓存文件 /Users/zkzhu/Project/PycharmProject/LMsEvaluation/datasets/imdb/cache_poisoning_train_max_sen_lenNone.pt 存在，直接载入缓存文件！\n",
      "[\u001b[032m2024-04-12 13:36:11,254\u001b[0m INFO] data_helpers 数据预处理一共耗时1.323s\n",
      "[\u001b[032m2024-04-12 13:36:11,254\u001b[0m INFO] data_helpers  ## 索引预处理缓存文件的参数为：['max_sen_len']\n",
      "[\u001b[032m2024-04-12 13:36:11,255\u001b[0m INFO] data_helpers 缓存文件 /Users/zkzhu/Project/PycharmProject/LMsEvaluation/datasets/imdb/cache_val_max_sen_lenNone.pt 存在，直接载入缓存文件！\n",
      "[\u001b[032m2024-04-12 13:36:12,338\u001b[0m INFO] data_helpers 数据预处理一共耗时1.083s\n",
      "[\u001b[032m2024-04-12 13:36:16,570\u001b[0m INFO] main Epoch: 0, Batch[0/25000], Train loss :3.162, Train acc: 0.000\n",
      "[\u001b[032m2024-04-12 13:36:16,573\u001b[0m INFO] main Epoch: 0, Train loss: 0.000, Epoch time = 4.231s\n",
      "[\u001b[032m2024-04-12 13:36:18,733\u001b[0m INFO] Bert ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现，如需使用torch框架中的MultiHeadAttention模块可通过config.__dict__['use_torch_multi_head'] = True实现\n",
      "[\u001b[032m2024-04-12 13:36:19,997\u001b[0m INFO] data_helpers  ## 索引预处理缓存文件的参数为：['max_sen_len']\n",
      "[\u001b[032m2024-04-12 13:36:19,998\u001b[0m INFO] data_helpers 缓存文件 /Users/zkzhu/Project/PycharmProject/LMsEvaluation/datasets/imdb/cache_test_max_sen_lenNone.pt 存在，直接载入缓存文件！\n",
      "[\u001b[032m2024-04-12 13:36:20,003\u001b[0m INFO] data_helpers 数据预处理一共耗时0.005s\n",
      "[\u001b[032m2024-04-12 13:36:20,004\u001b[0m INFO] data_helpers  ## 索引预处理缓存文件的参数为：['max_sen_len']\n",
      "[\u001b[032m2024-04-12 13:36:20,004\u001b[0m INFO] data_helpers 缓存文件 /Users/zkzhu/Project/PycharmProject/LMsEvaluation/datasets/imdb/cache_poisoning_train_max_sen_lenNone.pt 存在，直接载入缓存文件！\n",
      "[\u001b[032m2024-04-12 13:36:21,121\u001b[0m INFO] data_helpers 数据预处理一共耗时1.117s\n",
      "[\u001b[032m2024-04-12 13:36:21,121\u001b[0m INFO] data_helpers  ## 索引预处理缓存文件的参数为：['max_sen_len']\n",
      "[\u001b[032m2024-04-12 13:36:21,122\u001b[0m INFO] data_helpers 缓存文件 /Users/zkzhu/Project/PycharmProject/LMsEvaluation/datasets/imdb/cache_val_max_sen_lenNone.pt 存在，直接载入缓存文件！\n",
      "[\u001b[032m2024-04-12 13:36:22,225\u001b[0m INFO] data_helpers 数据预处理一共耗时1.104s\n",
      "[\u001b[032m2024-04-12 13:36:32,058\u001b[0m INFO] main Acc on test:0.000\n",
      "[\u001b[032m2024-04-12 13:36:32,230\u001b[0m INFO] TaskForSingleSentenceClassification 攻击结束\n",
      "[\u001b[032m2024-04-12 13:36:32,230\u001b[0m INFO] TaskForSingleSentenceClassification ==========================================================\n",
      "[\u001b[032m2024-04-12 13:36:34,358\u001b[0m INFO] Bert ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现，如需使用torch框架中的MultiHeadAttention模块可通过config.__dict__['use_torch_multi_head'] = True实现\n",
      "[\u001b[032m2024-04-12 13:36:35,211\u001b[0m INFO] data_helpers  ## 索引预处理缓存文件的参数为：['max_sen_len']\n",
      "[\u001b[032m2024-04-12 13:36:35,211\u001b[0m INFO] data_helpers 缓存文件 /Users/zkzhu/Project/PycharmProject/LMsEvaluation/datasets/imdb/cache_test_max_sen_lenNone.pt 存在，直接载入缓存文件！\n",
      "[\u001b[032m2024-04-12 13:36:35,216\u001b[0m INFO] data_helpers 数据预处理一共耗时0.005s\n",
      "[\u001b[032m2024-04-12 13:36:35,216\u001b[0m INFO] data_helpers  ## 索引预处理缓存文件的参数为：['max_sen_len']\n",
      "[\u001b[032m2024-04-12 13:36:35,217\u001b[0m INFO] data_helpers 缓存文件 /Users/zkzhu/Project/PycharmProject/LMsEvaluation/datasets/imdb/cache_poisoning_train_max_sen_lenNone.pt 存在，直接载入缓存文件！\n",
      "[\u001b[032m2024-04-12 13:36:35,946\u001b[0m INFO] data_helpers 数据预处理一共耗时0.729s\n",
      "[\u001b[032m2024-04-12 13:36:35,947\u001b[0m INFO] data_helpers  ## 索引预处理缓存文件的参数为：['max_sen_len']\n",
      "[\u001b[032m2024-04-12 13:36:35,947\u001b[0m INFO] data_helpers 缓存文件 /Users/zkzhu/Project/PycharmProject/LMsEvaluation/datasets/imdb/cache_val_max_sen_lenNone.pt 存在，直接载入缓存文件！\n",
      "[\u001b[032m2024-04-12 13:36:37,364\u001b[0m INFO] data_helpers 数据预处理一共耗时1.417s\n",
      "[\u001b[032m2024-04-12 13:36:41,891\u001b[0m INFO] TaskForSingleSentenceClassification Acc on test:0.000\n"
     ]
    }
   ],
   "source": [
    "%run main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cfac4e-bcb9-4262-9e2f-f88b7c06a92e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
