general:
  random_seed: 42
  use_gpu: True
  log_file_name: 'single'
  logs_save_dir: './logs'

LM_config:
  model: bert_base_uncased
  local_model: True

task_config:
  task: TaskForSingleSentenceClassification
  dataset: GLUE/cola
  local_dataset: True
  normal_training: True
  save_path: './cache/model_output'
  train_config:
    output_dir: './cache'
    num_train_epochs: 1
    per_device_train_batch_size: 16
    per_device_eval_batch_size: 64
    warmup_steps: 1000
    weight_decay: 0.01
    logging_dir: './logs'
    logging_steps: 1000
    run_name: 'my_experiment'
    report_to: "none"

attack_list:
  - attack_args:
      attack: False
      attack_type: AdvAttack
      attack_recipe: TextFoolerJin2019
      use_local_model: True
      use_local_tokenizer: True
      use_local_dataset: True
      model_name_or_path: "LMs/bert_base_uncased"
      #      model_name_or_path: "/cache/modelOutput/task4sst_model.pt"
      tokenizer_name_or_path: "LMs/bert_base_uncased"
      dataset_name_or_path: "datasets/imdb/train.txt"
      #      model_name_or_path: "LMs/bert_base_chinese"
      #      tokenizer_name_or_path: "LMs/bert_base_chinese"
      #      dataset_name_or_path: "datasets/ChineseNER/train.txt"
      attack_nums: 20
      display_full_info: True
      #      defender: None
      defender:
        num_epochs: 1
        num_clean_epochs: 1
        num_train_adv_examples: 1000
        learning_rate: 5e-5
        per_device_train_batch_size: 8
        gradient_accumulation_steps: 4
        log_to_tb: False
  - attack_args:
      attack: False
      attack_type: PoisoningAttack
      poisoning_rate: 0.3
      save_path: './attack/PoisoningAttack/model_output'
      train_config:
        output_dir: './attack/PoisoningAttack/cache'
        num_train_epochs: 1
        per_device_train_batch_size: 16
        per_device_eval_batch_size: 64
        warmup_steps: 1000
        weight_decay: 0.01
        logging_dir: './logs'
        logging_steps: 1000
        run_name: 'my_experiment'
        report_to: "none"
      defender: True
  - attack_args:
      attack: False
      attack_type: RLMI
      dataset_name: "emotion"
      model_name: "tinybert4"
      seed: 42
      ppo_config:
        mini_batch_size: 16
        batch_size: 16
        #        log_with: "wandb" # tensorboard, wandb
        log_with: None
        learning_rate: 1e-5
      seq_length: 20
      target_label: 0
      max_iterations: 2000
      min_input_length: 2
      max_input_length: 5
      num_generation: 1000
  - attack_args:
      attack: False
      attack_type: BackdoorAttack
      use_local_model: True
      model: "bert"
      model_name_or_path: "LMs/bert_base_uncased"
      poison_dataset: "sst-2"
      target_dataset: "sst-2"
      poisoner:
        "name": "badnets"
      train:
        "name": "base"
        "batch_size": 32
        "epochs": 1
      defender: "None"
      sample_metrics: [ 'ppl', 'use' ]
      display_full_info: True
  - attack_args:
      attack: False
      attack_type: ModelStealingAttack
      method: RS
      query_num: 320
      run_seed_arr: [ 56 ]
      pool_data_type: whole
      pool_data_source: wiki
      pool_subsize: -1
      prompt: None
      epsilon: -1
      initial_sample_method: random_sentence
      initial_drk_model: None
      al_sample_batch_num: -1
      al_sample_method: None
      defender: pruning # or output-perturb